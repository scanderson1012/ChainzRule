{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bcxj-QY6BE7",
        "outputId": "353a1983-aefc-475a-ef85-def28af16060"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nesn0jjqjlyq",
        "outputId": "335ac87c-c15a-480a-92e1-a096c60482b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "--- Data Loading ---\n",
            "Loading data from /content/drive/MyDrive/data/yelp_polar_glove_final_train.npy...\n",
            "Loading data from /content/drive/MyDrive/data/yelp_polar_glove_final_val.npy...\n",
            "Loading data from /content/drive/MyDrive/data/yelp_polar_glove_final_test.npy...\n",
            "Total Train samples: 418,600 | Val: 89,700 | Dim: 308\n",
            "Model initialized with in_features=308\n",
            "Epoch 01/250 | train_loss=0.63062 | val_acc=0.5235\n",
            "Epoch 02/250 | train_loss=0.31416 | val_acc=0.5235\n",
            "Epoch 03/250 | train_loss=0.27646 | val_acc=0.5235\n",
            "Epoch 04/250 | train_loss=0.25974 | val_acc=0.5294\n",
            "Epoch 05/250 | train_loss=0.24641 | val_acc=0.5660\n",
            "Epoch 06/250 | train_loss=0.23412 | val_acc=0.6354\n",
            "Epoch 07/250 | train_loss=0.22370 | val_acc=0.7152\n",
            "Epoch 08/250 | train_loss=0.21317 | val_acc=0.7836\n",
            "Epoch 09/250 | train_loss=0.20323 | val_acc=0.8327\n",
            "Epoch 10/250 | train_loss=0.19355 | val_acc=0.8646\n",
            "Epoch 11/250 | train_loss=0.18355 | val_acc=0.8808\n",
            "Epoch 12/250 | train_loss=0.17347 | val_acc=0.8877\n",
            "Epoch 13/250 | train_loss=0.16286 | val_acc=0.8886\n",
            "Epoch 14/250 | train_loss=0.15213 | val_acc=0.8865\n",
            "Epoch 15/250 | train_loss=0.14180 | val_acc=0.8835\n",
            "Epoch 16/250 | train_loss=0.13167 | val_acc=0.8815\n",
            "Epoch 17/250 | train_loss=0.12215 | val_acc=0.8793\n",
            "Epoch 18/250 | train_loss=0.11201 | val_acc=0.8784\n",
            "Epoch 19/250 | train_loss=0.10253 | val_acc=0.8786\n",
            "Epoch 20/250 | train_loss=0.09426 | val_acc=0.8804\n",
            "Epoch 21/250 | train_loss=0.08591 | val_acc=0.8831\n",
            "Epoch 22/250 | train_loss=0.07848 | val_acc=0.8867\n",
            "Epoch 23/250 | train_loss=0.07056 | val_acc=0.8916\n",
            "Epoch 24/250 | train_loss=0.06542 | val_acc=0.8973\n",
            "Epoch 25/250 | train_loss=0.05969 | val_acc=0.9032\n",
            "Epoch 26/250 | train_loss=0.05507 | val_acc=0.9095\n",
            "Epoch 27/250 | train_loss=0.05065 | val_acc=0.9159\n",
            "Epoch 28/250 | train_loss=0.04679 | val_acc=0.9220\n",
            "Epoch 29/250 | train_loss=0.04305 | val_acc=0.9273\n",
            "Epoch 30/250 | train_loss=0.04058 | val_acc=0.9322\n",
            "Epoch 31/250 | train_loss=0.03750 | val_acc=0.9370\n",
            "Epoch 32/250 | train_loss=0.03571 | val_acc=0.9418\n",
            "Epoch 33/250 | train_loss=0.03346 | val_acc=0.9455\n",
            "Epoch 34/250 | train_loss=0.03209 | val_acc=0.9485\n",
            "Epoch 35/250 | train_loss=0.03104 | val_acc=0.9511\n",
            "Epoch 36/250 | train_loss=0.02928 | val_acc=0.9530\n",
            "Epoch 37/250 | train_loss=0.02778 | val_acc=0.9546\n",
            "Epoch 38/250 | train_loss=0.02723 | val_acc=0.9559\n",
            "Epoch 39/250 | train_loss=0.02540 | val_acc=0.9569\n",
            "Epoch 40/250 | train_loss=0.02413 | val_acc=0.9576\n",
            "Epoch 41/250 | train_loss=0.02355 | val_acc=0.9581\n",
            "Epoch 42/250 | train_loss=0.02358 | val_acc=0.9588\n",
            "Epoch 43/250 | train_loss=0.02362 | val_acc=0.9594\n",
            "Epoch 44/250 | train_loss=0.02167 | val_acc=0.9596\n",
            "Epoch 45/250 | train_loss=0.02140 | val_acc=0.9600\n",
            "Epoch 46/250 | train_loss=0.02088 | val_acc=0.9601\n",
            "Epoch 47/250 | train_loss=0.02006 | val_acc=0.9603\n",
            "Epoch 48/250 | train_loss=0.01896 | val_acc=0.9605\n",
            "Epoch 49/250 | train_loss=0.01914 | val_acc=0.9606\n",
            "Epoch 50/250 | train_loss=0.01821 | val_acc=0.9608\n",
            "Epoch 51/250 | train_loss=0.01819 | val_acc=0.9609\n",
            "Epoch 52/250 | train_loss=0.01728 | val_acc=0.9607\n",
            "Epoch 53/250 | train_loss=0.01784 | val_acc=0.9605\n",
            "Epoch 54/250 | train_loss=0.01675 | val_acc=0.9607\n",
            "Epoch 55/250 | train_loss=0.01653 | val_acc=0.9607\n",
            "Epoch 56/250 | train_loss=0.01585 | val_acc=0.9605\n",
            "Epoch 57/250 | train_loss=0.01638 | val_acc=0.9604\n",
            "Epoch 58/250 | train_loss=0.01597 | val_acc=0.9605\n",
            "Epoch 59/250 | train_loss=0.01598 | val_acc=0.9605\n",
            "Epoch 60/250 | train_loss=0.01542 | val_acc=0.9605\n",
            "Epoch 61/250 | train_loss=0.01487 | val_acc=0.9606\n",
            "Epoch 62/250 | train_loss=0.01426 | val_acc=0.9607\n",
            "Epoch 63/250 | train_loss=0.01464 | val_acc=0.9606\n",
            "Epoch 64/250 | train_loss=0.01379 | val_acc=0.9608\n",
            "Epoch 65/250 | train_loss=0.01388 | val_acc=0.9606\n",
            "Epoch 66/250 | train_loss=0.01377 | val_acc=0.9607\n",
            "Epoch 67/250 | train_loss=0.01279 | val_acc=0.9606\n",
            "Epoch 68/250 | train_loss=0.01334 | val_acc=0.9607\n",
            "Epoch 69/250 | train_loss=0.01310 | val_acc=0.9606\n",
            "Epoch 70/250 | train_loss=0.01321 | val_acc=0.9606\n",
            "Epoch 71/250 | train_loss=0.01292 | val_acc=0.9605\n",
            "Epoch 72/250 | train_loss=0.01279 | val_acc=0.9607\n",
            "Epoch 73/250 | train_loss=0.01238 | val_acc=0.9606\n",
            "Epoch 74/250 | train_loss=0.01206 | val_acc=0.9606\n",
            "Epoch 75/250 | train_loss=0.01172 | val_acc=0.9606\n",
            "Epoch 76/250 | train_loss=0.01147 | val_acc=0.9606\n",
            "Early stopping.\n",
            "\n",
            "=== Training Complete ===\n",
            "Test acc@0.5: 0.9612\n",
            "Test acc@val-threshold (t=0.19): 0.9619\n"
          ]
        }
      ],
      "source": [
        "# train_chainrule_head.py\n",
        "# Train ChainRule head on frozen Hierarchical embeddings (300D), using combined feature+label NPY files\n",
        "# assuming files are in the CURRENT WORKING DIRECTORY (e.g., where this script is run from).\n",
        "import os, math, time, random\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_avg_fn\n",
        "import json\n",
        "\n",
        "# ============ Config ============\n",
        "# --- Data Paths: These files are assumed to be in the current working directory. ---\n",
        "FEATURE_TRAIN    = \"/content/drive/MyDrive/data/yelp_polar_glove_final_train.npy\"\n",
        "FEATURE_VAL      = \"/content/drive/MyDrive/data/yelp_polar_glove_final_val.npy\"\n",
        "FEATURE_TEST     = \"/content/drive/MyDrive/data/yelp_polar_glove_final_test.npy\"\n",
        "# NOTE: The last column of these files is assumed to contain the rating score (0-4).\n",
        "\n",
        "TARGET_MIN, TARGET_MAX = 0.0, 1.0\n",
        "\n",
        "BATCH_TRAIN = 4096\n",
        "BATCH_TEST  = 6144\n",
        "EPOCHS      = 250\n",
        "LR          = 4e-4\n",
        "WDECAY      = 6e-4\n",
        "DREG        = 1e-5\n",
        "PATIENCE    = 25\n",
        "USE_AMP     = True\n",
        "hidden_features = 3000\n",
        "num_layers  = 2\n",
        "degrees     = 2\n",
        "dropout     = 0.05\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if DEVICE.type == \"cuda\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "SEED = 1337\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# ============ Metrics ============\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return float(np.mean(np.abs(y_true - y_pred)))\n",
        "\n",
        "def qwk(y_true, y_pred, min_rating=0, max_rating=4):\n",
        "    y_true = np.asarray(y_true, dtype=int)\n",
        "    y_pred = np.asarray(y_pred, dtype=int)\n",
        "    M = max_rating - min_rating + 1\n",
        "    O = np.zeros((M, M), dtype=np.float64)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        if min_rating <= t <= max_rating and min_rating <= p <= max_rating:\n",
        "            O[t - min_rating, p - min_rating] += 1.0\n",
        "    act_hist = O.sum(axis=1)\n",
        "    pred_hist = O.sum(axis=0)\n",
        "    E = np.outer(act_hist, pred_hist) / max(1.0, O.sum())\n",
        "    W = np.zeros((M, M))\n",
        "    for i in range(M):\n",
        "        for j in range(M):\n",
        "            W[i, j] = ((i - j) ** 2) / ((M - 1) ** 2)\n",
        "    num = (W * O).sum()\n",
        "    den = (W * E).sum() if (W * E).sum() != 0 else 1.0\n",
        "    return 1.0 - num / den\n",
        "\n",
        "def round_clip(x):\n",
        "    return (x >= 0.5).astype(int)\n",
        "\n",
        "def apply_thresholds(p_cont, ts):\n",
        "    \"\"\"ts = [t1,t2,t3,t4] on 0..4; returns integer 0..4 via np.digitize.\"\"\"\n",
        "    ts = np.asarray(ts, dtype=np.float32)\n",
        "    return np.digitize(p_cont, ts).astype(int)\n",
        "\n",
        "def greedy_search_thresholds(y_true_int, p_cont, metric=\"qwk\",\n",
        "                             start_ts=(0.5,1.5,2.5,3.5),\n",
        "                             sweeps=3, step=0.10, margin=0.40):\n",
        "    \"\"\"Coordinate-descent tuner for 4 thresholds on 0..4.\"\"\"\n",
        "    ts = np.array(start_ts, dtype=np.float32)\n",
        "\n",
        "    def score_with(ts_local):\n",
        "        pr = apply_thresholds(p_cont, ts_local)\n",
        "        if metric == \"acc\":\n",
        "            return (pr == y_true_int).mean()\n",
        "        else:\n",
        "            return qwk(y_true_int, pr)\n",
        "\n",
        "    best_score = score_with(ts)\n",
        "    for _ in range(sweeps):\n",
        "        for i in range(len(ts)):\n",
        "            low = (ts[i-1] + 1e-3) if i > 0 else 0.0\n",
        "            high = (ts[i+1] - 1e-3) if i < 3 else 4.0\n",
        "            cand_grid = np.arange(max(low, ts[i]-margin),\n",
        "                                  min(high, ts[i]+margin)+1e-9, step, dtype=np.float32)\n",
        "            local_best = ts[i]; local_best_score = best_score\n",
        "            for c in cand_grid:\n",
        "                ts_try = ts.copy(); ts_try[i] = c\n",
        "                s = score_with(ts_try)\n",
        "                if s > local_best_score:\n",
        "                    local_best_score = s\n",
        "                    local_best = c\n",
        "            ts[i] = local_best\n",
        "            best_score = local_best_score\n",
        "    return ts.tolist(), float(best_score)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _collect_preds_cont(model, loader):\n",
        "    \"\"\"Return (y_true_int, y_pred_cont_0to4) from a loader (uses EMA if present).\"\"\"\n",
        "    eval_model = model.module if isinstance(model, AveragedModel) else model\n",
        "    eval_model.eval()\n",
        "    ys_true, ys_pred = [], []\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE, non_blocking=True)\n",
        "        if xb.ndim != 2 or xb.shape[-1] != in_dim:\n",
        "            xb = xb.view(-1, in_dim)\n",
        "        out, _ = eval_model(xb)\n",
        "        yhat_s = torch.sigmoid(out)\n",
        "        yhat   = TARGET_MIN + (TARGET_MAX - TARGET_MIN) * yhat_s\n",
        "        ys_true.append(yb.numpy())\n",
        "        ys_pred.append(yhat.squeeze(-1).cpu().numpy())\n",
        "    y = np.concatenate(ys_true).astype(int)\n",
        "    p = np.concatenate(ys_pred).astype(np.float32)\n",
        "    return y, p\n",
        "\n",
        "# ============ ChainRule Model ============\n",
        "class PolyLayerStable(nn.Module):\n",
        "    def __init__(self, features: int, degree: int):\n",
        "        super().__init__()\n",
        "        self.features = features\n",
        "        self.degree = degree\n",
        "        self.raw_coeffs = nn.Parameter(torch.randn(features, degree + 1) * 0.05)\n",
        "        self.gamma = nn.Parameter(torch.zeros(features))\n",
        "        self.scale = 0.5\n",
        "    def forward(self, h, dh):\n",
        "        coeffs = self.scale * torch.tanh(self.raw_coeffs)\n",
        "        # polynomial forward\n",
        "        powers = [torch.ones_like(h)]\n",
        "        for _ in range(1, self.degree + 1):\n",
        "            powers.append(powers[-1] * h)\n",
        "        H = torch.stack(powers, dim=-1)         # (B,F,deg+1)\n",
        "        y = torch.sum(H * coeffs, dim=-1)       # (B,F)\n",
        "        # derivative part\n",
        "        if self.degree >= 1:\n",
        "            ar = torch.arange(1, self.degree + 1, device=h.device, dtype=h.dtype)\n",
        "            deriv_coeffs = coeffs[:, 1:] * ar    # (F,deg)\n",
        "            dlocal = torch.sum(H[..., :-1] * deriv_coeffs, dim=-1) # (B,F)\n",
        "        else:\n",
        "            dlocal = torch.zeros_like(h)\n",
        "        y = y + 0.1 * self.gamma * dh\n",
        "        dh_out = dlocal * dh\n",
        "        return y, dh_out\n",
        "\n",
        "class ChainRulePolyNetStable(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=256, num_layers=2, degree=3):\n",
        "        super().__init__()\n",
        "        self.in_map = nn.Linear(in_features, hidden_features)\n",
        "        self.in_norm = nn.LayerNorm(hidden_features)         # NEW\n",
        "        self.in_drop = nn.Dropout(p=dropout)                     # NEW\n",
        "        self.layers = nn.ModuleList([PolyLayerStable(hidden_features, degree) for _ in range(num_layers)])\n",
        "        self.out_map = nn.Linear(hidden_features, 1)\n",
        "    def forward(self, x):\n",
        "        h = self.in_map(x)\n",
        "        h = self.in_drop(self.in_norm(torch.nn.functional.gelu(h)))  # NEW\n",
        "        dh = torch.ones_like(h)\n",
        "        reg = 0.0\n",
        "        for layer in self.layers:\n",
        "            h, dh = layer(h, dh)\n",
        "            reg = reg + torch.mean(dh ** 2)\n",
        "        y = self.out_map(h) # (B,1)\n",
        "        return y, reg / max(1, len(self.layers))\n",
        "\n",
        "# ============ Datasets (Combined Feature/Label Logic) ============\n",
        "\n",
        "def _safe_load_npy(path: str):\n",
        "    \"\"\"\n",
        "    Try memory-mapped load first; if the file was saved/truncated oddly,\n",
        "    fall back to a normal load (no mmap). Raises a clear error if both fail.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return np.load(path, mmap_mode=\"r\")\n",
        "    except Exception as e_mmap:\n",
        "        print(f\"⚠️ mmap load failed for '{path}': {e_mmap}\\n   → Retrying without mmap...\")\n",
        "        try:\n",
        "            return np.load(path)  # no mmap\n",
        "        except Exception as e_plain:\n",
        "            raise RuntimeError(\n",
        "                f\"Failed to load '{path}' even without mmap. \"\n",
        "                f\"The file may be truncated or corrupted. Original error: {e_plain}\"\n",
        "            )\n",
        "\n",
        "class CombinedNumpyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for pre-split NumPy files where the final column is the label.\n",
        "    Accepts either a proper 2D numeric array (N, D+1) or a flat 1D numeric array\n",
        "    that can be reshaped into (N, D+1) when divisible by a plausible column count.\n",
        "    \"\"\"\n",
        "    def __init__(self, full_path: str, prefer_cols: int | None = None):\n",
        "        print(f\"Loading data from {full_path}...\")\n",
        "        if not os.path.exists(full_path):\n",
        "            raise FileNotFoundError(f\"File not found: {full_path}\")\n",
        "\n",
        "        data = _safe_load_npy(full_path)\n",
        "\n",
        "        if data.dtype == np.object_:\n",
        "            raise ValueError(\n",
        "                f\"{full_path} is an object array (likely pickled). \"\n",
        "                f\"Re-save as numeric (e.g., float32) with shape (N, D+1).\"\n",
        "            )\n",
        "\n",
        "        # Case 1: Already 2D\n",
        "        if data.ndim == 2:\n",
        "            self.features = np.array(data[:, :-1], dtype=np.float32, copy=True)  # was: np.asarray(...)\n",
        "            self.labels   = np.array(data[:,  -1], dtype=np.float32, copy=True).reshape(-1)\n",
        "            self.dim = self.features.shape[1]\n",
        "            return\n",
        "\n",
        "        # Case 2: Flat 1D → try to infer and reshape\n",
        "        if data.ndim == 1:\n",
        "            total = data.size\n",
        "            candidates = []\n",
        "\n",
        "            # Prefer a known columns count if provided (e.g., 300D + 1 label = 301)\n",
        "            if prefer_cols is not None and total % prefer_cols == 0:\n",
        "                candidates.append(prefer_cols)\n",
        "\n",
        "            # Otherwise search a reasonable range of divisors\n",
        "            if not candidates:\n",
        "                for c in range(32, 4097):\n",
        "                    if total % c == 0:\n",
        "                        candidates.append(c)\n",
        "\n",
        "            if not candidates:\n",
        "                raise ValueError(\n",
        "                    f\"{full_path}: flat size {total:,} has no reasonable divisors in [32, 4096]; \"\n",
        "                    f\"cannot infer (features+label) column count.\"\n",
        "                )\n",
        "\n",
        "            # Pick the candidate closest to common shapes like 301/309\n",
        "            candidates.sort(key=lambda c: min(abs(c - 301), abs(c - 309)))\n",
        "            cols = candidates[0]\n",
        "            n = total // cols\n",
        "            print(f\"ℹ️  Inferred shape for {full_path}: ({n:,}, {cols}) → features={cols-1}, label=1.\")\n",
        "\n",
        "            data2d = np.array(data, dtype=np.float32, copy=True).reshape(n, cols)\n",
        "            self.features = data2d[:, :-1]\n",
        "            self.labels   = data2d[:,  -1].reshape(-1)\n",
        "            self.dim = self.features.shape[1]\n",
        "            return\n",
        "\n",
        "        raise ValueError(f\"{full_path} has ndim={data.ndim}; expected 1D (flat) or 2D numeric array.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.features.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.features[idx])\n",
        "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return x, y\n",
        "\n",
        "def build_loaders_from_files():\n",
        "    \"\"\"Loads pre-split data from combined NPY files and creates DataLoaders.\"\"\"\n",
        "    print(f\"--- Data Loading ---\")\n",
        "\n",
        "    # If you know your exact columns (e.g., 300D + 1 label = 301), set prefer_cols=301.\n",
        "    # Otherwise leave None to auto-infer for each file.\n",
        "    prefer_cols = None  # e.g., set to 301 if you want to enforce 300D+label\n",
        "\n",
        "    try:\n",
        "        train_dataset = CombinedNumpyDataset(FEATURE_TRAIN, prefer_cols=prefer_cols)\n",
        "        val_dataset   = CombinedNumpyDataset(FEATURE_VAL,   prefer_cols=prefer_cols)\n",
        "\n",
        "        if os.path.exists(FEATURE_TEST):\n",
        "            test_dataset = CombinedNumpyDataset(FEATURE_TEST, prefer_cols=prefer_cols)\n",
        "        else:\n",
        "            test_dataset = None\n",
        "            print(f\"Warning: Test file {FEATURE_TEST} not found. Test set will be skipped.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            f\"Error loading combined NPY files. Ensure files are numeric (features + label) and not truncated. \"\n",
        "            f\"Error: {e}\"\n",
        "        )\n",
        "\n",
        "    in_dim = train_dataset.dim\n",
        "    if val_dataset.dim != in_dim or (test_dataset and test_dataset.dim != in_dim):\n",
        "        raise ValueError(\n",
        "            f\"Embedding dimensions must match across splits: \"\n",
        "            f\"train={in_dim}, val={val_dataset.dim}, \"\n",
        "            f\"test={(test_dataset.dim if test_dataset else '—')}.\"\n",
        "        )\n",
        "\n",
        "    print(f\"Total Train samples: {len(train_dataset):,} | Val: {len(val_dataset):,} | Dim: {in_dim}\")\n",
        "\n",
        "    pin = (DEVICE.type == \"cuda\")\n",
        "    # cap at 2 when on this runtime\n",
        "    n_workers = 2 if pin else 0\n",
        "    persistent = (n_workers > 0)\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_TRAIN, shuffle=True,\n",
        "        pin_memory=pin, num_workers=n_workers, persistent_workers=persistent\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=BATCH_TEST, shuffle=False,\n",
        "        pin_memory=pin, num_workers=n_workers, persistent_workers=persistent\n",
        "    )\n",
        "    test_loader = None\n",
        "    if test_dataset:\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, batch_size=BATCH_TEST, shuffle=False,\n",
        "            pin_memory=pin, num_workers=n_workers, persistent_workers=persistent\n",
        "        )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, in_dim\n",
        "\n",
        "# ============ Training / Eval ============\n",
        "\n",
        "def train_loop(\n",
        "    model, train_loader, val_loader,\n",
        "    epochs=EPOCHS, lr=LR, wdecay=WDECAY, dreg=DREG,\n",
        "    use_amp=USE_AMP, grad_clip=0.5, train_probe_batches=10\n",
        "):\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=(use_amp and DEVICE.type == \"cuda\"))\n",
        "    opt = AdamW(model.parameters(), lr=lr, weight_decay=wdecay, fused=torch.cuda.is_available())\n",
        "\n",
        "    total_steps = max(1, epochs * len(train_loader))\n",
        "    warmup_steps = max(10, len(train_loader))\n",
        "\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return float(step + 1) / warmup_steps\n",
        "        t = (step - warmup_steps) / max(1, (total_steps - warmup_steps))\n",
        "        return 0.01 + 0.99 * 0.5 * (1 + math.cos(math.pi * t))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "    ema = AveragedModel(model, avg_fn=get_ema_avg_fn(0.999))\n",
        "\n",
        "    best_val = -math.inf\n",
        "    best = None\n",
        "    best_ema = None\n",
        "    no_improve = 0\n",
        "    global in_dim\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        tot, n = 0.0, 0\n",
        "\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(DEVICE, non_blocking=True)\n",
        "            if xb.ndim != 2 or xb.shape[-1] != in_dim:\n",
        "                xb = xb.view(-1, in_dim)\n",
        "\n",
        "            # IMPORTANT: BCEWithLogits expects float targets\n",
        "            ys = yb.to(DEVICE, non_blocking=True).float().unsqueeze(-1)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            if use_amp and DEVICE.type == \"cuda\":\n",
        "                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                    logits, reg = model(xb)\n",
        "                    reg = torch.clamp(reg, max=10.0)\n",
        "                    loss = nn.functional.binary_cross_entropy_with_logits(logits, ys) + dreg * reg\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(opt)\n",
        "                if grad_clip is not None:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                logits, reg = model(xb)\n",
        "                reg = torch.clamp(reg, max=10.0)\n",
        "                loss = nn.functional.binary_cross_entropy_with_logits(logits, ys) + dreg * reg\n",
        "                loss.backward()\n",
        "                if grad_clip is not None:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
        "                opt.step()\n",
        "\n",
        "            ema.update_parameters(model)\n",
        "            scheduler.step()\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            tot += loss.item() * bs\n",
        "            n += bs\n",
        "\n",
        "        # ---- fast train accuracy probe (first N batches only) ----\n",
        "        train_metrics = eval_loop(ema, train_loader, max_batches=train_probe_batches)\n",
        "        train_acc_fast = train_metrics[\"acc_round\"]\n",
        "\n",
        "        val_metrics = eval_loop(ema, val_loader)     # thresholds=None => 0.5\n",
        "        val_acc_05 = val_metrics[\"acc_round\"]\n",
        "\n",
        "        print(f\"Epoch {ep:02d}/{epochs} | train_loss={tot/max(1,n):.5f} | val_acc={val_acc_05:.4f}\")\n",
        "\n",
        "        metric = val_acc_05\n",
        "\n",
        "        if metric > best_val + 1e-6:\n",
        "            best_val = metric\n",
        "            best = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "            best_ema = {k: v.cpu() for k, v in ema.module.state_dict().items()}\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= PATIENCE:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    if best is not None:\n",
        "        model.load_state_dict(best)\n",
        "        if best_ema is not None:\n",
        "            ema.module.load_state_dict(best_ema)\n",
        "\n",
        "    return ema\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loop(model, loader, thresholds=None, max_batches=None, return_best_thresh=False):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        dict with:\n",
        "            - acc_round: accuracy using threshold(s) (0.5 if thresholds is None)\n",
        "            - best_thresh (optional): best scalar threshold on this loader\n",
        "            - best_acc (optional): best accuracy achieved by scalar threshold sweep\n",
        "    \"\"\"\n",
        "    eval_model = model.module if isinstance(model, AveragedModel) else model\n",
        "    eval_model.eval()\n",
        "\n",
        "    ys_true, ys_pred = [], []\n",
        "\n",
        "    for bidx, (xb, yb) in enumerate(loader):\n",
        "        if (max_batches is not None) and (bidx >= max_batches):\n",
        "            break\n",
        "\n",
        "        xb = xb.to(DEVICE, non_blocking=True)\n",
        "        if xb.ndim != 2 or xb.shape[-1] != in_dim:\n",
        "            xb = xb.view(-1, in_dim)\n",
        "\n",
        "        out, _ = eval_model(xb)                 # logits\n",
        "        p = torch.sigmoid(out).squeeze(-1)      # probs in [0,1]\n",
        "\n",
        "        ys_true.append(yb.detach().cpu().numpy())\n",
        "        ys_pred.append(p.detach().cpu().numpy())\n",
        "\n",
        "    y = np.concatenate(ys_true).astype(int)\n",
        "    p = np.concatenate(ys_pred)\n",
        "\n",
        "    if thresholds is None:\n",
        "        pr = (p >= 0.5).astype(int)\n",
        "    elif isinstance(thresholds, (float, int)):\n",
        "        pr = (p >= thresholds).astype(int)\n",
        "    else:\n",
        "        pr = apply_thresholds(p, thresholds)\n",
        "\n",
        "    acc = float(np.mean(pr == y))\n",
        "    out_dict = {\"acc_round\": acc}\n",
        "\n",
        "    # Optional: find best scalar threshold on this loader\n",
        "    if return_best_thresh:\n",
        "        best_t, best_acc = 0.5, acc\n",
        "        # sweep 0.05..0.95\n",
        "        for t in np.linspace(0.05, 0.95, 91):\n",
        "            pr_t = (p >= t).astype(int)\n",
        "            acc_t = float(np.mean(pr_t == y))\n",
        "            if acc_t > best_acc:\n",
        "                best_acc, best_t = acc_t, float(t)\n",
        "        out_dict[\"best_thresh\"] = best_t\n",
        "        out_dict[\"best_acc\"] = best_acc\n",
        "\n",
        "    return out_dict\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Device: {DEVICE.type}\")\n",
        "\n",
        "    # --- Initial File Check ---\n",
        "    if not os.path.exists(FEATURE_TRAIN):\n",
        "        raise FileNotFoundError(f\"Training file '{FEATURE_TRAIN}' not found. Ensure it is in the current working directory.\")\n",
        "    if not os.path.exists(FEATURE_VAL):\n",
        "        raise FileNotFoundError(f\"Validation file '{FEATURE_VAL}' not found. Ensure it is in the current working directory.\")\n",
        "\n",
        "    train_loader, val_loader, test_loader, in_dim = build_loaders_from_files()\n",
        "\n",
        "    # Initialize model using the dynamically determined input dimension\n",
        "    model = ChainRulePolyNetStable(in_features=in_dim, hidden_features=hidden_features, num_layers=num_layers, degree=degrees)\n",
        "    print(f\"Model initialized with in_features={in_dim}\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model = train_loop(model, train_loader, val_loader)\n",
        "    dur = time.time() - t0\n",
        "\n",
        "    print(\"\\n=== Training Complete ===\")\n",
        "\n",
        "val_metrics = eval_loop(model, val_loader, return_best_thresh=True)\n",
        "t_star = val_metrics[\"best_thresh\"]\n",
        "\n",
        "if test_loader:\n",
        "    test_05 = eval_loop(model, test_loader)[\"acc_round\"]\n",
        "    test_t  = eval_loop(model, test_loader, thresholds=t_star)[\"acc_round\"]\n",
        "    print(f\"Test acc@0.5: {test_05:.4f}\")\n",
        "    print(f\"Test acc@val-threshold (t={t_star:.2f}): {test_t:.4f}\")\n",
        "else:\n",
        "    print(\"Test metrics skipped because the test data file was not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjbczkN96OOl",
        "outputId": "fab24a62-4981-4c1f-f9b3-80407a5ef0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Parameter Counts ===\n",
            "Total trainable params: 960,001\n",
            "  in_map  : 927,000\n",
            "  in_norm : 6,000\n",
            "  layers  : 24,000\n",
            "  out_map : 3,001\n",
            "Embedded params (poly raw_coeffs + gamma): 24,000\n",
            "External frozen embeddings in .npy (not part of model): 0 trainable params\n",
            "Your KPI is 16.078536917166687\n"
          ]
        }
      ],
      "source": [
        "# ---- Parameter accounting ----\n",
        "\n",
        "def count_params(model):\n",
        "    # Access the underlying model if it's an AveragedModel\n",
        "    if isinstance(model, AveragedModel):\n",
        "        model = model.module\n",
        "\n",
        "    total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Per-submodule tallies\n",
        "    sub = {}\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        top = name.split('.')[0]  # e.g., 'in_map', 'in_norm', 'layers', 'out_map'\n",
        "        sub[top] = sub.get(top, 0) + p.numel()\n",
        "\n",
        "    # Poly-layer \"embedded\" params = all raw_coeffs + gamma across layers\n",
        "    poly_embedded = 0\n",
        "    # Check if the model has layers before iterating\n",
        "    if hasattr(model, 'layers'):\n",
        "        for l in model.layers:\n",
        "            # Check if the layer has raw_coeffs and gamma before accessing\n",
        "            if hasattr(l, 'raw_coeffs'):\n",
        "                 poly_embedded += l.raw_coeffs.numel()\n",
        "            if hasattr(l, 'gamma'):\n",
        "                 poly_embedded += l.gamma.numel()\n",
        "\n",
        "\n",
        "    return total, sub, poly_embedded\n",
        "\n",
        "total, sub_breakdown, poly_embedded = count_params(model)\n",
        "\n",
        "print(\"\\n=== Parameter Counts ===\")\n",
        "print(f\"Total trainable params: {total:,}\")\n",
        "for k in sorted(sub_breakdown.keys()):\n",
        "    print(f\"  {k:8s}: {sub_breakdown[k]:,}\")\n",
        "print(f\"Embedded params (poly raw_coeffs + gamma): {poly_embedded:,}\")\n",
        "print(\"External frozen embeddings in .npy (not part of model): 0 trainable params\")\n",
        "print(f\"Your KPI is {test_t*100/np.log10(total)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a6kfJRYIhBt",
        "outputId": "6a92a6b6-2fad-44a5-8d5c-c1c2fa5342db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Calculating F1, Precision, and Recall on Test Set ===\n",
            "Error: 'rating_thresholds.json' not found. Ensure the main training script ran successfully and saved the thresholds.\n",
            "Skipping F1/P/R calculation: Thresholds file or Test Loader not available.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "# Assuming the necessary utility functions like apply_thresholds and\n",
        "# _collect_preds_cont, along with global variables (model, test_loader,\n",
        "# TARGET_MIN, TARGET_MAX) are defined from the main script execution.\n",
        "\n",
        "# Re-defining necessary utility from user's script for robust execution:\n",
        "def apply_thresholds(p_cont, ts):\n",
        "    \"\"\"ts = [t] on 0..1; returns integer 0..1 via np.digitize.\"\"\"\n",
        "    ts = np.asarray(ts, dtype=np.float32)\n",
        "    return np.digitize(p_cont, ts).astype(int)\n",
        "\n",
        "# ============ FINAL EVALUATION CELL ============\n",
        "print(\"\\n=== Calculating F1, Precision, and Recall on Test Set ===\")\n",
        "\n",
        "# 1. Load Tuned Thresholds (saved during the main script's final steps)\n",
        "try:\n",
        "    with open(\"rating_thresholds.json\", \"r\") as f:\n",
        "        threshold_data = json.load(f)\n",
        "        BEST_THRESHOLDS = threshold_data[\"thresholds\"]\n",
        "    print(f\"Loaded optimal thresholds: {[round(t, 3) for t in BEST_THRESHOLDS]}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'rating_thresholds.json' not found. Ensure the main training script ran successfully and saved the thresholds.\")\n",
        "    BEST_THRESHOLDS = None\n",
        "except Exception as e:\n",
        "    print(f\"Error loading thresholds: {e}\")\n",
        "    BEST_THRESHOLDS = None\n",
        "\n",
        "if BEST_THRESHOLDS and 'test_loader' in locals():\n",
        "    # 2. Collect True Labels and Continuous Predictions\n",
        "    # NOTE: This line requires the _collect_preds_cont function and the EMA model object ('model')\n",
        "    y_true_int, p_cont = _collect_preds_cont(model, test_loader)\n",
        "\n",
        "    # 3. Apply Tuned Thresholds to get Discrete Predictions (0, 1, 2, 3, 4)\n",
        "    y_pred_int = apply_thresholds(p_cont, BEST_THRESHOLDS)\n",
        "\n",
        "    # 4. Calculate Classification Metrics (Multi-class 0-4)\n",
        "    # Using 'weighted' average accounts for class imbalance\n",
        "    f1 = f1_score(y_true_int, y_pred_int, average='weighted')\n",
        "    precision = precision_score(y_true_int, y_pred_int, average='weighted')\n",
        "    recall = recall_score(y_true_int, y_pred_int, average='weighted')\n",
        "\n",
        "    print(\"\\n--- Weighted Classification Metrics ---\")\n",
        "    print(f\"Test Precision (Weighted): {precision:.4f}\")\n",
        "    print(f\"Test Recall (Weighted):    {recall:.4f}\")\n",
        "    print(f\"Test F1-Score (Weighted):  {f1:.4f}\")\n",
        "\n",
        "    # Optional: Detailed report for each class (0, 1, 2, 3, 4)\n",
        "    print(\"\\nDetailed Multi-Class Classification Report:\")\n",
        "    # Assuming TARGET_MIN and TARGET_MAX are available globally\n",
        "    target_names = [\"negative\", \"positive\"]\n",
        "    print(classification_report(y_true_int, y_pred_int, target_names=target_names))\n",
        "\n",
        "else:\n",
        "    print(\"Skipping F1/P/R calculation: Thresholds file or Test Loader not available.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}