{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWWxEpVC8VWY",
        "outputId": "1007a108-3581-44f1-9f63-4e97551bd0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nesn0jjqjlyq",
        "outputId": "d47e2a5e-3e52-448b-f07f-640cb891e0d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "--- Data Loading ---\n",
            "Loading data from /content/drive/MyDrive/data/ag_glove_train.npy...\n",
            "Loading data from /content/drive/MyDrive/data/ag_glove_val.npy...\n",
            "Loading data from /content/drive/MyDrive/data/ag_glove_test.npy...\n",
            "Total Train samples: 89,320 | Val: 19,140 | Dim: 308\n",
            "Model initialized with in_features=308\n",
            "Epoch 01/250 | train_loss=1.38272 | val_acc=0.2528\n",
            "Epoch 02/250 | train_loss=1.31120 | val_acc=0.3358\n",
            "Epoch 03/250 | train_loss=0.81106 | val_acc=0.2592\n",
            "Epoch 04/250 | train_loss=0.39330 | val_acc=0.2460\n",
            "Epoch 05/250 | train_loss=0.31228 | val_acc=0.2460\n",
            "Epoch 06/250 | train_loss=0.27862 | val_acc=0.2460\n",
            "Epoch 07/250 | train_loss=0.25733 | val_acc=0.2460\n",
            "Epoch 08/250 | train_loss=0.24072 | val_acc=0.2460\n",
            "Epoch 09/250 | train_loss=0.22768 | val_acc=0.2460\n",
            "Epoch 10/250 | train_loss=0.21503 | val_acc=0.2460\n",
            "Epoch 11/250 | train_loss=0.20341 | val_acc=0.2461\n",
            "Epoch 12/250 | train_loss=0.19261 | val_acc=0.2472\n",
            "Epoch 13/250 | train_loss=0.18238 | val_acc=0.2510\n",
            "Epoch 14/250 | train_loss=0.17180 | val_acc=0.2638\n",
            "Epoch 15/250 | train_loss=0.16223 | val_acc=0.2835\n",
            "Epoch 16/250 | train_loss=0.15324 | val_acc=0.3126\n",
            "Epoch 17/250 | train_loss=0.14446 | val_acc=0.3423\n",
            "Epoch 18/250 | train_loss=0.13490 | val_acc=0.3706\n",
            "Epoch 19/250 | train_loss=0.12614 | val_acc=0.3956\n",
            "Epoch 20/250 | train_loss=0.11706 | val_acc=0.4162\n",
            "Epoch 21/250 | train_loss=0.10907 | val_acc=0.4351\n",
            "Epoch 22/250 | train_loss=0.10128 | val_acc=0.4490\n",
            "Epoch 23/250 | train_loss=0.09328 | val_acc=0.4588\n",
            "Epoch 24/250 | train_loss=0.08647 | val_acc=0.4671\n",
            "Epoch 25/250 | train_loss=0.08002 | val_acc=0.4732\n",
            "Epoch 26/250 | train_loss=0.07427 | val_acc=0.4799\n",
            "Epoch 27/250 | train_loss=0.06989 | val_acc=0.4873\n",
            "Epoch 28/250 | train_loss=0.06500 | val_acc=0.4947\n",
            "Epoch 29/250 | train_loss=0.05947 | val_acc=0.5041\n",
            "Epoch 30/250 | train_loss=0.05539 | val_acc=0.5162\n",
            "Epoch 31/250 | train_loss=0.05123 | val_acc=0.5310\n",
            "Epoch 32/250 | train_loss=0.04908 | val_acc=0.5479\n",
            "Epoch 33/250 | train_loss=0.04569 | val_acc=0.5679\n",
            "Epoch 34/250 | train_loss=0.04237 | val_acc=0.5877\n",
            "Epoch 35/250 | train_loss=0.03958 | val_acc=0.6084\n",
            "Epoch 36/250 | train_loss=0.03702 | val_acc=0.6292\n",
            "Epoch 37/250 | train_loss=0.03518 | val_acc=0.6514\n",
            "Epoch 38/250 | train_loss=0.03384 | val_acc=0.6745\n",
            "Epoch 39/250 | train_loss=0.03130 | val_acc=0.6931\n",
            "Epoch 40/250 | train_loss=0.02939 | val_acc=0.7111\n",
            "Epoch 41/250 | train_loss=0.02814 | val_acc=0.7267\n",
            "Epoch 42/250 | train_loss=0.02705 | val_acc=0.7427\n",
            "Epoch 43/250 | train_loss=0.02643 | val_acc=0.7587\n",
            "Epoch 44/250 | train_loss=0.02453 | val_acc=0.7729\n",
            "Epoch 45/250 | train_loss=0.02211 | val_acc=0.7856\n",
            "Epoch 46/250 | train_loss=0.02113 | val_acc=0.7963\n",
            "Epoch 47/250 | train_loss=0.02221 | val_acc=0.8077\n",
            "Epoch 48/250 | train_loss=0.01932 | val_acc=0.8168\n",
            "Epoch 49/250 | train_loss=0.01851 | val_acc=0.8257\n",
            "Epoch 50/250 | train_loss=0.01833 | val_acc=0.8357\n",
            "Epoch 51/250 | train_loss=0.01679 | val_acc=0.8435\n",
            "Epoch 52/250 | train_loss=0.01616 | val_acc=0.8505\n",
            "Epoch 53/250 | train_loss=0.01581 | val_acc=0.8576\n",
            "Epoch 54/250 | train_loss=0.01488 | val_acc=0.8638\n",
            "Epoch 55/250 | train_loss=0.01501 | val_acc=0.8705\n",
            "Epoch 56/250 | train_loss=0.01308 | val_acc=0.8771\n",
            "Epoch 57/250 | train_loss=0.01373 | val_acc=0.8825\n",
            "Epoch 58/250 | train_loss=0.01406 | val_acc=0.8868\n",
            "Epoch 59/250 | train_loss=0.01438 | val_acc=0.8913\n",
            "Epoch 60/250 | train_loss=0.01257 | val_acc=0.8945\n",
            "Epoch 61/250 | train_loss=0.01195 | val_acc=0.8980\n",
            "Epoch 62/250 | train_loss=0.01175 | val_acc=0.9011\n",
            "Epoch 63/250 | train_loss=0.01125 | val_acc=0.9041\n",
            "Epoch 64/250 | train_loss=0.01151 | val_acc=0.9073\n",
            "Epoch 65/250 | train_loss=0.01133 | val_acc=0.9095\n",
            "Epoch 66/250 | train_loss=0.01068 | val_acc=0.9114\n",
            "Epoch 67/250 | train_loss=0.01098 | val_acc=0.9141\n",
            "Epoch 68/250 | train_loss=0.01058 | val_acc=0.9154\n",
            "Epoch 69/250 | train_loss=0.01008 | val_acc=0.9175\n",
            "Epoch 70/250 | train_loss=0.01057 | val_acc=0.9186\n",
            "Epoch 71/250 | train_loss=0.00891 | val_acc=0.9204\n",
            "Epoch 72/250 | train_loss=0.00892 | val_acc=0.9217\n",
            "Epoch 73/250 | train_loss=0.00930 | val_acc=0.9226\n",
            "Epoch 74/250 | train_loss=0.00858 | val_acc=0.9233\n",
            "Epoch 75/250 | train_loss=0.00805 | val_acc=0.9241\n",
            "Epoch 76/250 | train_loss=0.00867 | val_acc=0.9250\n",
            "Epoch 77/250 | train_loss=0.00852 | val_acc=0.9253\n",
            "Epoch 78/250 | train_loss=0.00753 | val_acc=0.9255\n",
            "Epoch 79/250 | train_loss=0.00827 | val_acc=0.9265\n",
            "Epoch 80/250 | train_loss=0.00750 | val_acc=0.9269\n",
            "Epoch 81/250 | train_loss=0.00748 | val_acc=0.9276\n",
            "Epoch 82/250 | train_loss=0.00720 | val_acc=0.9283\n",
            "Epoch 83/250 | train_loss=0.00679 | val_acc=0.9287\n",
            "Epoch 84/250 | train_loss=0.00689 | val_acc=0.9297\n",
            "Epoch 85/250 | train_loss=0.00707 | val_acc=0.9305\n",
            "Epoch 86/250 | train_loss=0.00728 | val_acc=0.9308\n",
            "Epoch 87/250 | train_loss=0.00644 | val_acc=0.9318\n",
            "Epoch 88/250 | train_loss=0.00633 | val_acc=0.9325\n",
            "Epoch 89/250 | train_loss=0.00637 | val_acc=0.9332\n",
            "Epoch 90/250 | train_loss=0.00624 | val_acc=0.9334\n",
            "Epoch 91/250 | train_loss=0.00649 | val_acc=0.9336\n",
            "Epoch 92/250 | train_loss=0.00661 | val_acc=0.9339\n",
            "Epoch 93/250 | train_loss=0.00674 | val_acc=0.9344\n",
            "Epoch 94/250 | train_loss=0.00591 | val_acc=0.9347\n",
            "Epoch 95/250 | train_loss=0.00601 | val_acc=0.9348\n",
            "Epoch 96/250 | train_loss=0.00616 | val_acc=0.9349\n",
            "Epoch 97/250 | train_loss=0.00613 | val_acc=0.9350\n",
            "Epoch 98/250 | train_loss=0.00521 | val_acc=0.9352\n",
            "Epoch 99/250 | train_loss=0.00576 | val_acc=0.9353\n",
            "Epoch 100/250 | train_loss=0.00551 | val_acc=0.9353\n",
            "Epoch 101/250 | train_loss=0.00536 | val_acc=0.9350\n",
            "Epoch 102/250 | train_loss=0.00565 | val_acc=0.9351\n",
            "Epoch 103/250 | train_loss=0.00491 | val_acc=0.9351\n",
            "Epoch 104/250 | train_loss=0.00504 | val_acc=0.9350\n",
            "Epoch 105/250 | train_loss=0.00481 | val_acc=0.9351\n",
            "Epoch 106/250 | train_loss=0.00465 | val_acc=0.9351\n",
            "Epoch 107/250 | train_loss=0.00458 | val_acc=0.9354\n",
            "Epoch 108/250 | train_loss=0.00526 | val_acc=0.9353\n",
            "Epoch 109/250 | train_loss=0.00445 | val_acc=0.9354\n",
            "Epoch 110/250 | train_loss=0.00451 | val_acc=0.9353\n",
            "Epoch 111/250 | train_loss=0.00454 | val_acc=0.9355\n",
            "Epoch 112/250 | train_loss=0.00465 | val_acc=0.9354\n",
            "Epoch 113/250 | train_loss=0.00393 | val_acc=0.9358\n",
            "Epoch 114/250 | train_loss=0.00423 | val_acc=0.9357\n",
            "Epoch 115/250 | train_loss=0.00434 | val_acc=0.9358\n",
            "Epoch 116/250 | train_loss=0.00379 | val_acc=0.9358\n",
            "Epoch 117/250 | train_loss=0.00395 | val_acc=0.9357\n",
            "Epoch 118/250 | train_loss=0.00349 | val_acc=0.9359\n",
            "Epoch 119/250 | train_loss=0.00416 | val_acc=0.9359\n",
            "Epoch 120/250 | train_loss=0.00456 | val_acc=0.9361\n",
            "Epoch 121/250 | train_loss=0.00429 | val_acc=0.9362\n",
            "Epoch 122/250 | train_loss=0.00360 | val_acc=0.9361\n",
            "Epoch 123/250 | train_loss=0.00384 | val_acc=0.9365\n",
            "Epoch 124/250 | train_loss=0.00398 | val_acc=0.9365\n",
            "Epoch 125/250 | train_loss=0.00395 | val_acc=0.9367\n",
            "Epoch 126/250 | train_loss=0.00393 | val_acc=0.9369\n",
            "Epoch 127/250 | train_loss=0.00310 | val_acc=0.9370\n",
            "Epoch 128/250 | train_loss=0.00333 | val_acc=0.9370\n",
            "Epoch 129/250 | train_loss=0.00321 | val_acc=0.9374\n",
            "Epoch 130/250 | train_loss=0.00358 | val_acc=0.9374\n",
            "Epoch 131/250 | train_loss=0.00328 | val_acc=0.9374\n",
            "Epoch 132/250 | train_loss=0.00379 | val_acc=0.9371\n",
            "Epoch 133/250 | train_loss=0.00328 | val_acc=0.9371\n",
            "Epoch 134/250 | train_loss=0.00344 | val_acc=0.9371\n",
            "Epoch 135/250 | train_loss=0.00367 | val_acc=0.9370\n",
            "Epoch 136/250 | train_loss=0.00350 | val_acc=0.9370\n",
            "Epoch 137/250 | train_loss=0.00294 | val_acc=0.9370\n",
            "Epoch 138/250 | train_loss=0.00322 | val_acc=0.9372\n",
            "Epoch 139/250 | train_loss=0.00314 | val_acc=0.9371\n",
            "Epoch 140/250 | train_loss=0.00307 | val_acc=0.9370\n",
            "Epoch 141/250 | train_loss=0.00292 | val_acc=0.9371\n",
            "Epoch 142/250 | train_loss=0.00277 | val_acc=0.9370\n",
            "Epoch 143/250 | train_loss=0.00256 | val_acc=0.9370\n",
            "Epoch 144/250 | train_loss=0.00314 | val_acc=0.9371\n",
            "Epoch 145/250 | train_loss=0.00262 | val_acc=0.9370\n",
            "Epoch 146/250 | train_loss=0.00315 | val_acc=0.9371\n",
            "Epoch 147/250 | train_loss=0.00281 | val_acc=0.9373\n",
            "Epoch 148/250 | train_loss=0.00313 | val_acc=0.9371\n",
            "Epoch 149/250 | train_loss=0.00259 | val_acc=0.9372\n",
            "Epoch 150/250 | train_loss=0.00282 | val_acc=0.9371\n",
            "Epoch 151/250 | train_loss=0.00272 | val_acc=0.9369\n",
            "Epoch 152/250 | train_loss=0.00252 | val_acc=0.9369\n",
            "Epoch 153/250 | train_loss=0.00243 | val_acc=0.9368\n",
            "Epoch 154/250 | train_loss=0.00252 | val_acc=0.9369\n",
            "Epoch 155/250 | train_loss=0.00236 | val_acc=0.9369\n",
            "Epoch 156/250 | train_loss=0.00209 | val_acc=0.9369\n",
            "Epoch 157/250 | train_loss=0.00258 | val_acc=0.9369\n",
            "Epoch 158/250 | train_loss=0.00232 | val_acc=0.9368\n",
            "Epoch 159/250 | train_loss=0.00272 | val_acc=0.9368\n",
            "Epoch 160/250 | train_loss=0.00223 | val_acc=0.9369\n",
            "Epoch 161/250 | train_loss=0.00219 | val_acc=0.9368\n",
            "Epoch 162/250 | train_loss=0.00218 | val_acc=0.9367\n",
            "Epoch 163/250 | train_loss=0.00260 | val_acc=0.9366\n",
            "Epoch 164/250 | train_loss=0.00235 | val_acc=0.9366\n",
            "Epoch 165/250 | train_loss=0.00284 | val_acc=0.9365\n",
            "Epoch 166/250 | train_loss=0.00265 | val_acc=0.9365\n",
            "Epoch 167/250 | train_loss=0.00262 | val_acc=0.9364\n",
            "Epoch 168/250 | train_loss=0.00202 | val_acc=0.9364\n",
            "Epoch 169/250 | train_loss=0.00200 | val_acc=0.9364\n",
            "Epoch 170/250 | train_loss=0.00239 | val_acc=0.9365\n",
            "Early stopping.\n",
            "\n",
            "=== Training Complete ===\n",
            "\n",
            "=== Test Metrics ===\n",
            "Accuracy: 0.9366\n",
            "Total train time: 554.77s\n"
          ]
        }
      ],
      "source": [
        "# train_chainrule_head.py\n",
        "# Train ChainRule head on frozen Hierarchical embeddings (300D), using combined feature+label NPY files\n",
        "# assuming files are in the CURRENT WORKING DIRECTORY (e.g., where this script is run from).\n",
        "import os, math, time, random\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.optim.swa_utils import AveragedModel, get_ema_avg_fn\n",
        "import json\n",
        "\n",
        "# ============ Config ============\n",
        "# --- Data Paths: These files are assumed to be in the current working directory. ---\n",
        "FEATURE_TRAIN    = \"/content/drive/MyDrive/data/ag_glove_train.npy\"\n",
        "FEATURE_VAL      = \"/content/drive/MyDrive/data/ag_glove_val.npy\"\n",
        "FEATURE_TEST     = \"/content/drive/MyDrive/data/ag_glove_test.npy\"\n",
        "# NOTE: The last column of these files is assumed to contain the rating score (0-4).\n",
        "\n",
        "NUM_CLASSES = 4     # AG News\n",
        "# NUM_CLASSES = 14  # DBpedia\n",
        "\n",
        "\n",
        "BATCH_TRAIN = 4096\n",
        "BATCH_TEST  = 6144\n",
        "EPOCHS      = 250\n",
        "LR          = 3e-4\n",
        "WDECAY      = 6e-4 #8e-4\n",
        "DREG        = 4e-5\n",
        "PATIENCE    = 40\n",
        "USE_AMP     = False\n",
        "hidden_features = 5000\n",
        "num_layers  = 2\n",
        "degrees     = 2\n",
        "dropout     = 0.05\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if DEVICE.type == \"cuda\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    try:\n",
        "        torch.set_float32_matmul_precision(\"high\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "SEED = 1337\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "# ============ Metrics ============\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ============ ChainRule Model ============\n",
        "class PolyLayerStable(nn.Module):\n",
        "    def __init__(self, features: int, degree: int):\n",
        "        super().__init__()\n",
        "        self.features = features\n",
        "        self.degree = degree\n",
        "        self.raw_coeffs = nn.Parameter(torch.randn(features, degree + 1) * 0.05)\n",
        "        self.gamma = nn.Parameter(torch.zeros(features))\n",
        "        self.scale = 0.5\n",
        "    def forward(self, h, dh):\n",
        "        coeffs = self.scale * torch.tanh(self.raw_coeffs)\n",
        "        # polynomial forward\n",
        "        powers = [torch.ones_like(h)]\n",
        "        for _ in range(1, self.degree + 1):\n",
        "            powers.append(powers[-1] * h)\n",
        "        H = torch.stack(powers, dim=-1)         # (B,F,deg+1)\n",
        "        y = torch.sum(H * coeffs, dim=-1)       # (B,F)\n",
        "        # derivative part\n",
        "        if self.degree >= 1:\n",
        "            ar = torch.arange(1, self.degree + 1, device=h.device, dtype=h.dtype)\n",
        "            deriv_coeffs = coeffs[:, 1:] * ar    # (F,deg)\n",
        "            dlocal = torch.sum(H[..., :-1] * deriv_coeffs, dim=-1) # (B,F)\n",
        "        else:\n",
        "            dlocal = torch.zeros_like(h)\n",
        "        y = y + 0.1 * self.gamma * dh\n",
        "        dh_out = dlocal * dh\n",
        "        return y, dh_out\n",
        "\n",
        "class ChainRulePolyNetStable(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=256, num_layers=2, degree=3):\n",
        "        super().__init__()\n",
        "        self.in_map = nn.Linear(in_features, hidden_features)\n",
        "        self.in_norm = nn.LayerNorm(hidden_features)         # NEW\n",
        "        self.in_drop = nn.Dropout(p=dropout)                     # NEW\n",
        "        self.layers = nn.ModuleList([PolyLayerStable(hidden_features, degree) for _ in range(num_layers)])\n",
        "        self.out_map = nn.Linear(hidden_features, NUM_CLASSES)\n",
        "    def forward(self, x):\n",
        "        h = self.in_map(x)\n",
        "        h = self.in_drop(self.in_norm(torch.nn.functional.gelu(h)))  # NEW\n",
        "        dh = torch.ones_like(h)\n",
        "        reg = 0.0\n",
        "        for layer in self.layers:\n",
        "            h, dh = layer(h, dh)\n",
        "            reg = reg + torch.mean(dh ** 2)\n",
        "        y = self.out_map(h) # (B,1)\n",
        "        return y, reg / max(1, len(self.layers))\n",
        "\n",
        "# ============ Datasets (Combined Feature/Label Logic) ============\n",
        "\n",
        "def _safe_load_npy(path: str):\n",
        "    \"\"\"\n",
        "    Try memory-mapped load first; if the file was saved/truncated oddly,\n",
        "    fall back to a normal load (no mmap). Raises a clear error if both fail.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return np.load(path, mmap_mode=\"r\")\n",
        "    except Exception as e_mmap:\n",
        "        print(f\"⚠️ mmap load failed for '{path}': {e_mmap}\\n   → Retrying without mmap...\")\n",
        "        try:\n",
        "            return np.load(path)  # no mmap\n",
        "        except Exception as e_plain:\n",
        "            raise RuntimeError(\n",
        "                f\"Failed to load '{path}' even without mmap. \"\n",
        "                f\"The file may be truncated or corrupted. Original error: {e_plain}\"\n",
        "            )\n",
        "\n",
        "class CombinedNumpyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for pre-split NumPy files where the final column is the label.\n",
        "    Accepts either a proper 2D numeric array (N, D+1) or a flat 1D numeric array\n",
        "    that can be reshaped into (N, D+1) when divisible by a plausible column count.\n",
        "    \"\"\"\n",
        "    def __init__(self, full_path: str, prefer_cols: int | None = None):\n",
        "        print(f\"Loading data from {full_path}...\")\n",
        "        if not os.path.exists(full_path):\n",
        "            raise FileNotFoundError(f\"File not found: {full_path}\")\n",
        "\n",
        "        data = _safe_load_npy(full_path)\n",
        "\n",
        "        if data.dtype == np.object_:\n",
        "            raise ValueError(\n",
        "                f\"{full_path} is an object array (likely pickled). \"\n",
        "                f\"Re-save as numeric (e.g., float32) with shape (N, D+1).\"\n",
        "            )\n",
        "\n",
        "        # Case 1: Already 2D\n",
        "        if data.ndim == 2:\n",
        "            self.features = np.array(data[:, :-1], dtype=np.float32, copy=True)  # was: np.asarray(...)\n",
        "            self.labels   = np.array(data[:,  -1], dtype=np.float32, copy=True).reshape(-1)\n",
        "            self.dim = self.features.shape[1]\n",
        "            return\n",
        "\n",
        "        # Case 2: Flat 1D → try to infer and reshape\n",
        "        if data.ndim == 1:\n",
        "            total = data.size\n",
        "            candidates = []\n",
        "\n",
        "            # Prefer a known columns count if provided (e.g., 300D + 1 label = 301)\n",
        "            if prefer_cols is not None and total % prefer_cols == 0:\n",
        "                candidates.append(prefer_cols)\n",
        "\n",
        "            # Otherwise search a reasonable range of divisors\n",
        "            if not candidates:\n",
        "                for c in range(32, 4097):\n",
        "                    if total % c == 0:\n",
        "                        candidates.append(c)\n",
        "\n",
        "            if not candidates:\n",
        "                raise ValueError(\n",
        "                    f\"{full_path}: flat size {total:,} has no reasonable divisors in [32, 4096]; \"\n",
        "                    f\"cannot infer (features+label) column count.\"\n",
        "                )\n",
        "\n",
        "            # Pick the candidate closest to common shapes like 301/309\n",
        "            candidates.sort(key=lambda c: min(abs(c - 301), abs(c - 309)))\n",
        "            cols = candidates[0]\n",
        "            n = total // cols\n",
        "            print(f\"ℹ️  Inferred shape for {full_path}: ({n:,}, {cols}) → features={cols-1}, label=1.\")\n",
        "\n",
        "            data2d = np.array(data, dtype=np.float32, copy=True).reshape(n, cols)\n",
        "            self.features = data2d[:, :-1]\n",
        "            self.labels   = data2d[:,  -1].reshape(-1)\n",
        "            self.dim = self.features.shape[1]\n",
        "            return\n",
        "\n",
        "        raise ValueError(f\"{full_path} has ndim={data.ndim}; expected 1D (flat) or 2D numeric array.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.features.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.features[idx])\n",
        "        y = torch.tensor(int(self.labels[idx]), dtype=torch.long) # used to be y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "def build_loaders_from_files():\n",
        "    \"\"\"Loads pre-split data from combined NPY files and creates DataLoaders.\"\"\"\n",
        "    print(f\"--- Data Loading ---\")\n",
        "\n",
        "    # If you know your exact columns (e.g., 300D + 1 label = 301), set prefer_cols=301.\n",
        "    # Otherwise leave None to auto-infer for each file.\n",
        "    prefer_cols = None  # e.g., set to 301 if you want to enforce 300D+label\n",
        "\n",
        "    try:\n",
        "        train_dataset = CombinedNumpyDataset(FEATURE_TRAIN, prefer_cols=prefer_cols)\n",
        "        val_dataset   = CombinedNumpyDataset(FEATURE_VAL,   prefer_cols=prefer_cols)\n",
        "\n",
        "        if os.path.exists(FEATURE_TEST):\n",
        "            test_dataset = CombinedNumpyDataset(FEATURE_TEST, prefer_cols=prefer_cols)\n",
        "        else:\n",
        "            test_dataset = None\n",
        "            print(f\"Warning: Test file {FEATURE_TEST} not found. Test set will be skipped.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\n",
        "            f\"Error loading combined NPY files. Ensure files are numeric (features + label) and not truncated. \"\n",
        "            f\"Error: {e}\"\n",
        "        )\n",
        "\n",
        "    in_dim = train_dataset.dim\n",
        "    if val_dataset.dim != in_dim or (test_dataset and test_dataset.dim != in_dim):\n",
        "        raise ValueError(\n",
        "            f\"Embedding dimensions must match across splits: \"\n",
        "            f\"train={in_dim}, val={val_dataset.dim}, \"\n",
        "            f\"test={(test_dataset.dim if test_dataset else '—')}.\"\n",
        "        )\n",
        "\n",
        "    print(f\"Total Train samples: {len(train_dataset):,} | Val: {len(val_dataset):,} | Dim: {in_dim}\")\n",
        "\n",
        "    pin = (DEVICE.type == \"cuda\")\n",
        "    # cap at 2 when on this runtime\n",
        "    n_workers = 2 if pin else 0\n",
        "    persistent = (n_workers > 0)\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_TRAIN, shuffle=True,\n",
        "        pin_memory=pin, num_workers=n_workers, persistent_workers=persistent\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=BATCH_TEST, shuffle=False,\n",
        "        pin_memory=pin, num_workers=n_workers, persistent_workers=persistent\n",
        "    )\n",
        "    test_loader = None\n",
        "    if test_dataset:\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset, batch_size=BATCH_TEST, shuffle=False,\n",
        "            pin_memory=pin, num_workers=n_workers, persistent_workers=persistent\n",
        "        )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, in_dim\n",
        "\n",
        "# ============ Training / Eval ============\n",
        "def train_loop(model, train_loader, val_loader, epochs=EPOCHS, lr=LR, wdecay=WDECAY, dreg=DREG, use_amp=USE_AMP):\n",
        "    model = model.to(DEVICE)\n",
        "    # CHANGED: modern GradScaler API\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=(use_amp and DEVICE.type == \"cuda\"))  # NEW\n",
        "\n",
        "    # CHANGED: fused AdamW when available\n",
        "    opt = AdamW(model.parameters(), lr=lr, weight_decay=wdecay, fused=torch.cuda.is_available())  # NEW\n",
        "\n",
        "#     opt = AdamW(\n",
        "#     model.parameters(),\n",
        "#     lr=lr,\n",
        "#     weight_decay=wdecay,\n",
        "#     betas=(0.9, 0.98),   # ← KEY CHANGE\n",
        "#     eps=1e-8,\n",
        "#     fused=torch.cuda.is_available()\n",
        "# )\n",
        "\n",
        "    # NEW: warmup + cosine schedule\n",
        "    total_steps = max(1, epochs * len(train_loader))\n",
        "    warmup_steps = max(10, len(train_loader))\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return float(step + 1) / warmup_steps\n",
        "        t = (step - warmup_steps) / max(1, (total_steps - warmup_steps))\n",
        "        return 0.01 + 0.99 * 0.5 * (1 + math.cos(math.pi * t))\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)  # NEW\n",
        "\n",
        "    # NEW: EMA tracking\n",
        "    ema = AveragedModel(model, avg_fn=get_ema_avg_fn(0.999))\n",
        "\n",
        "    best_val = -math.inf\n",
        "    best = None\n",
        "    best_ema = None\n",
        "    no_improve = 0\n",
        "    global in_dim\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    step_idx = 0  # NEW\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        tot, n = 0.0, 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb = xb.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            # Ensure (B, in_dim)\n",
        "            if xb.ndim != 2 or xb.shape[-1] != in_dim:\n",
        "                xb = xb.view(-1, in_dim)\n",
        "\n",
        "            # scale labels to [0,1]\n",
        "            yb = yb.to(DEVICE, non_blocking=True).long()\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            if use_amp and DEVICE.type == \"cuda\":\n",
        "                # CHANGED: modern autocast\n",
        "                with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                    logits, reg = model(xb)\n",
        "                    reg = torch.clamp(reg, max=10.0)\n",
        "                    loss = criterion(logits, yb) + dreg * reg\n",
        "                scaler.scale(loss).backward()\n",
        "                # NEW: clip grads in AMP path (after unscale)\n",
        "                scaler.unscale_(opt)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "                scaler.step(opt); scaler.update()\n",
        "            else:\n",
        "                logits, reg = model(xb)\n",
        "                reg = torch.clamp(reg, max=10.0)\n",
        "                loss = criterion(logits, yb) + dreg * reg\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5) # ------------------------------------------------------------------------\n",
        "                opt.step()\n",
        "\n",
        "            # NEW: EMA + scheduler step\n",
        "            ema.update_parameters(model)\n",
        "            scheduler.step()\n",
        "            step_idx += 1\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            tot += loss.item() * bs\n",
        "            n += bs\n",
        "\n",
        "        val_acc = eval_loop(ema, val_loader)[\"acc\"]\n",
        "        print(f\"Epoch {ep:02d}/{epochs} | train_loss={tot/max(1,n):.5f} | val_acc={val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val + 1e-6:\n",
        "            best_val = val_acc\n",
        "            best = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "            best_ema = {k: v.cpu() for k, v in ema.module.state_dict().items()}\n",
        "            no_improve = 0\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= PATIENCE:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    if best is not None:\n",
        "        model.load_state_dict(best)\n",
        "        if best_ema is not None:\n",
        "            ema.module.load_state_dict(best_ema)  # NEW\n",
        "\n",
        "    # Return the EMA-smoothed model for downstream eval\n",
        "    return ema  # CHANGED\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loop(model, loader):\n",
        "    eval_model = model.module if isinstance(model, AveragedModel) else model\n",
        "    eval_model.eval()\n",
        "\n",
        "    ys_true, ys_pred = [], []\n",
        "\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        # Ensure (B, in_dim)\n",
        "        if xb.ndim != 2 or xb.shape[-1] != in_dim:\n",
        "            xb = xb.view(-1, in_dim)\n",
        "\n",
        "        logits, _ = eval_model(xb)          # shape: (B, C)\n",
        "        pred = logits.argmax(dim=1)         # shape: (B,)\n",
        "        ys_true.append(yb.numpy())\n",
        "        ys_pred.append(pred.cpu().numpy())\n",
        "\n",
        "    y = np.concatenate(ys_true)\n",
        "    p = np.concatenate(ys_pred)\n",
        "\n",
        "    acc = float(np.mean(p == y))\n",
        "    return {\"acc\": acc}\n",
        "\n",
        "# ============ Main ============\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Device: {DEVICE.type}\")\n",
        "\n",
        "    # --- Initial File Check ---\n",
        "    if not os.path.exists(FEATURE_TRAIN):\n",
        "        raise FileNotFoundError(f\"Training file '{FEATURE_TRAIN}' not found. Ensure it is in the current working directory.\")\n",
        "    if not os.path.exists(FEATURE_VAL):\n",
        "        raise FileNotFoundError(f\"Validation file '{FEATURE_VAL}' not found. Ensure it is in the current working directory.\")\n",
        "\n",
        "    train_loader, val_loader, test_loader, in_dim = build_loaders_from_files()\n",
        "\n",
        "    # Initialize model using the dynamically determined input dimension\n",
        "    model = ChainRulePolyNetStable(\n",
        "        in_features=in_dim,\n",
        "        hidden_features=hidden_features,\n",
        "        num_layers=num_layers,\n",
        "        degree=degrees\n",
        "    )\n",
        "    print(f\"Model initialized with in_features={in_dim}\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model = train_loop(model, train_loader, val_loader)\n",
        "    dur = time.time() - t0\n",
        "\n",
        "    print(\"\\n=== Training Complete ===\")\n",
        "\n",
        "    if test_loader:\n",
        "        metrics = eval_loop(model, test_loader)\n",
        "        print(\"\\n=== Test Metrics ===\")\n",
        "        print(f\"Accuracy: {metrics['acc']:.4f}\")\n",
        "    else:\n",
        "        print(\"Test metrics skipped because the test data file was not found.\")\n",
        "\n",
        "    print(f\"Total train time: {dur:.2f}s\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Parameter accounting ----\n",
        "def count_params(model):\n",
        "    # Access the underlying model if it's an AveragedModel\n",
        "    if isinstance(model, AveragedModel):\n",
        "        model = model.module\n",
        "\n",
        "    total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Per-submodule tallies\n",
        "    sub = {}\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        top = name.split('.')[0]  # e.g., 'in_map', 'in_norm', 'layers', 'out_map'\n",
        "        sub[top] = sub.get(top, 0) + p.numel()\n",
        "\n",
        "    # Poly-layer \"embedded\" params = all raw_coeffs + gamma across layers\n",
        "    poly_embedded = 0\n",
        "    # Check if the model has layers before iterating\n",
        "    if hasattr(model, 'layers'):\n",
        "        for l in model.layers:\n",
        "            # Check if the layer has raw_coeffs and gamma before accessing\n",
        "            if hasattr(l, 'raw_coeffs'):\n",
        "                 poly_embedded += l.raw_coeffs.numel()\n",
        "            if hasattr(l, 'gamma'):\n",
        "                 poly_embedded += l.gamma.numel()\n",
        "\n",
        "\n",
        "    return total, sub, poly_embedded\n",
        "\n",
        "total, sub_breakdown, poly_embedded = count_params(model)\n",
        "\n",
        "print(\"\\n=== Parameter Counts ===\")\n",
        "print(f\"Total trainable params: {total:,}\")\n",
        "for k in sorted(sub_breakdown.keys()):\n",
        "    print(f\"  {k:8s}: {sub_breakdown[k]:,}\")\n",
        "print(f\"Embedded params (poly raw_coeffs + gamma): {poly_embedded:,}\")\n",
        "print(\"External frozen embeddings in .npy (not part of model): 0 trainable params\")\n",
        "print(f\"Your KPI is {metrics['acc']*100/np.log10(total)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjbczkN96OOl",
        "outputId": "afa97f46-9c3d-4abb-ae99-6e094483569f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Parameter Counts ===\n",
            "Total trainable params: 1,615,004\n",
            "  in_map  : 1,545,000\n",
            "  in_norm : 10,000\n",
            "  layers  : 40,000\n",
            "  out_map : 20,004\n",
            "Embedded params (poly raw_coeffs + gamma): 40,000\n",
            "External frozen embeddings in .npy (not part of model): 0 trainable params\n",
            "Your KPI is 15.086121664346816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# ============ FINAL EVALUATION CELL (NOMINAL) ============\n",
        "print(\"\\n=== Calculating F1, Precision, and Recall on Test Set (Nominal) ===\")\n",
        "\n",
        "def collect_preds_nominal(model, loader):\n",
        "    \"\"\"\n",
        "    Collect true labels and predicted class indices for nominal multiclass classification.\n",
        "    EMA-aware.\n",
        "    \"\"\"\n",
        "    eval_model = model.module if isinstance(model, torch.optim.swa_utils.AveragedModel) else model\n",
        "    eval_model.eval()\n",
        "\n",
        "    ys_true = []\n",
        "    ys_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(DEVICE, non_blocking=True)\n",
        "\n",
        "            # ensure correct shape\n",
        "            if xb.ndim != 2 or xb.shape[-1] != in_dim:\n",
        "                xb = xb.view(-1, in_dim)\n",
        "\n",
        "            logits, _ = eval_model(xb)          # (B, C)\n",
        "            preds = torch.argmax(logits, dim=1)  # (B,)\n",
        "\n",
        "            ys_true.append(yb.cpu().numpy())\n",
        "            ys_pred.append(preds.cpu().numpy())\n",
        "\n",
        "    y_true = np.concatenate(ys_true).astype(int)\n",
        "    y_pred = np.concatenate(ys_pred).astype(int)\n",
        "\n",
        "    return y_true, y_pred\n",
        "\n",
        "\n",
        "# ---- Run evaluation ----\n",
        "if 'test_loader' in locals():\n",
        "\n",
        "    y_true_int, y_pred_int = collect_preds_nominal(model, test_loader)\n",
        "\n",
        "    # Weighted metrics (appropriate even if perfectly balanced)\n",
        "    f1 = f1_score(y_true_int, y_pred_int, average=\"weighted\")\n",
        "    precision = precision_score(y_true_int, y_pred_int, average=\"weighted\")\n",
        "    recall = recall_score(y_true_int, y_pred_int, average=\"weighted\")\n",
        "\n",
        "    print(\"\\n--- Weighted Classification Metrics ---\")\n",
        "    print(f\"Test Precision (Weighted): {precision:.4f}\")\n",
        "    print(f\"Test Recall (Weighted):    {recall:.4f}\")\n",
        "    print(f\"Test F1-Score (Weighted):  {f1:.4f}\")\n",
        "\n",
        "    print(\"\\nDetailed Multi-Class Classification Report:\")\n",
        "    target_names = [str(i) for i in range(NUM_CLASSES)]\n",
        "    print(classification_report(y_true_int, y_pred_int, target_names=target_names))\n",
        "\n",
        "else:\n",
        "    print(\"Skipping evaluation: test_loader not available.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQPcSwAejTTt",
        "outputId": "3f65d966-3595-4ed4-9ff0-6ab1b35d79c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Calculating F1, Precision, and Recall on Test Set (Nominal) ===\n",
            "\n",
            "--- Weighted Classification Metrics ---\n",
            "Test Precision (Weighted): 0.9370\n",
            "Test Recall (Weighted):    0.9366\n",
            "Test F1-Score (Weighted):  0.9365\n",
            "\n",
            "Detailed Multi-Class Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.93      0.94      4857\n",
            "           1       0.98      0.99      0.98      4749\n",
            "           2       0.92      0.89      0.91      4707\n",
            "           3       0.90      0.94      0.92      4827\n",
            "\n",
            "    accuracy                           0.94     19140\n",
            "   macro avg       0.94      0.94      0.94     19140\n",
            "weighted avg       0.94      0.94      0.94     19140\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}